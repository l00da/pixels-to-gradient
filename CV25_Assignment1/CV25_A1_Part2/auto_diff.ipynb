{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bac08d-fe8f-4fe0-aa65-814e869d1c08",
   "metadata": {},
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "In this assignment, we will be implementing a class that supports [Automatic Differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) on scalar values.\n",
    "\n",
    "Automatic Differentiation fundamentally relies on **chain rule** and **partial derivatives of composite functions** $f(x) = a(b(c(x))) \\implies f' = a' b' c'$ for accumulating gradients.\n",
    "\n",
    "As we saw in the lecture there are two approaches to implementing Automatic Differentation. **forward-mode/right-to-left** and **reverse-mode/left-to-right**.\n",
    "\n",
    "### Forward-mode\n",
    "\n",
    "We calculate the partial derivatives of functions in the order the functions are composed. For a function $f(x) = a(b(c(x)))$ this involves\n",
    "1. Evaluate $c(x)$ and calculate $c' = \\frac{\\delta c}{\\delta x}$.\n",
    "2. Evaluate $b(c(x))$. Calculate $b' = \\frac{\\delta b}{\\delta c}$ and use it to calculate $\\frac{\\delta b}{\\delta x} = b' c' = \\frac{\\delta b}{\\delta c} \\frac{\\delta c}{\\delta x}$. $c(x)$ and $c'$ are available from step 1.\n",
    "3. Similarly evaluate $a(b(c(x)))$ and calculate $\\frac{\\delta b}{\\delta x}$. Use $b(c(x))$ and $b'c'$ from the previous step.\n",
    "4. We now have $\\frac{\\delta f}{\\delta x}$.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We will do a minimal implementation that only supports a single scalar variable, you can refer to [micrograd](https://github.com/karpathy/micrograd/tree/master) to get an idea of backward-mode implementation with multi-variable support by storing the computation DAG.\n",
    "\n",
    "Complete the Python class \"ValueFwd\" to support forward-mode Automatic Differentiation as explained above.\n",
    "1. The class should store two values, the value of the variable/constant and its gradient w.r.t the independent variable. (hint: the gradient should be initialized to 0 for constants and 1 for variables, we will only have a single variable).\n",
    "2. **ValueFwd** should support basic arithmetic operations, similar to **torch.Tensor**. Complete the class methods to add this functionality to ValueFwd. \"\\_\\_add\\_\\_\" is already implemented to give you an idea.\n",
    "3. **TODO** Complete the implementation of ValueFwd and run the test code. play around with the input to test function, you are also free to implement mode test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28332d6a-857a-42a7-94aa-9c0181e1cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import sin, cos\n",
    "import math\n",
    "from numpy import log\n",
    "from typing import List, Tuple\n",
    "from numpy.testing import assert_almost_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca601b9-cb3e-459b-bc23-847c4b08b600",
   "metadata": {},
   "source": [
    "# forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "161f3fea-1724-4e00-bc91-64ddced37337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFwd:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "    def __init__(self, data, diff = 0):\n",
    "        self.v = data\n",
    "        self.d = diff\n",
    "\n",
    "    def __add__(l, r):\n",
    "        \"\"\" support for + on ValueFwd datatype.\n",
    "\n",
    "         l + r returns f, such that f.v = l.v + r.v and f.d = f' = df/dx.\n",
    "        \n",
    "        Args:\n",
    "            l (ValueFwd): ValueFwd to the left of +.\n",
    "            r (ValueFwd): ValueFwd to the right of +.\n",
    "\n",
    "        Returns:\n",
    "            ValueFwd with updated value and gradient.\n",
    "        \"\"\"\n",
    "        return ValueFwd(l.v + r.v, l.d + r.d)\n",
    "\n",
    "    def __sub__(l, r):\n",
    "        \"\"\" support for - on ValueFwd datatype.\n",
    "\n",
    "         l - r returns f, such that f.v = l.v - r.v and f.d = f' = df/dx.\n",
    "        \n",
    "        Args:\n",
    "            l (ValueFwd): ValueFwd to the left of -.\n",
    "            r (ValueFwd): ValueFwd to the right of -.\n",
    "\n",
    "        Returns:\n",
    "            ValueFwd with updated value and gradient.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __mul__(l, r):\n",
    "        \"\"\" support for * on ValueFwd datatype.\n",
    "\n",
    "         l * r returns f, such that f.v = l.v * r.v and f.d = f' = df/dx.\n",
    "        \n",
    "        Args:\n",
    "            l (ValueFwd): ValueFwd to the left of *.\n",
    "            r (ValueFwd): ValueFwd to the right of *.\n",
    "\n",
    "        Returns:\n",
    "            ValueFwd with updated value and gradient.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __truediv__(l, r):\n",
    "        \"\"\" support for / on ValueFwd datatype.\n",
    "\n",
    "         l / r returns f, such that f.v = l.v / r.v and f.d = f' = df/dx.\n",
    "        \n",
    "        Args:\n",
    "            l (ValueFwd): ValueFwd to the left of /.\n",
    "            r (ValueFwd): ValueFwd to the right of /.\n",
    "\n",
    "        Returns:\n",
    "            ValueFwd with updated value and gradient.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sin(self):\n",
    "        \"\"\" support for self.sin() on ValueFwd datatype.\n",
    "\n",
    "        Args:\n",
    "            self (ValueFwd): ValueFwd.\n",
    "\n",
    "        Returns:\n",
    "            returns ValueFwd f, such that f.v = sin(self.v) and f.d = f' = df/dx\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def cos(self):\n",
    "        \"\"\" support for self.cos() on ValueFwd datatype.\n",
    "\n",
    "        Args:\n",
    "            self (ValueFwd): ValueFwd.\n",
    "\n",
    "        Returns:\n",
    "            returns ValueFwd f, such that f.v = cos(self.v) and f.d = f' = df/dx\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def exp(self, b):\n",
    "        \"\"\" support for b^self on ValueFwd datatype.\n",
    "\n",
    "            self.exp(math.e) = e^self\n",
    "            \n",
    "        Args:\n",
    "            self (ValueFwd): ValueFwd.\n",
    "            b (Number): A numerical value.\n",
    "\n",
    "        Returns:\n",
    "            returns ValueFwd f, such that f.v = b^(self.v) and f.d = f' = df/dx\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def pow(self, p):\n",
    "        \"\"\" support for pow on ValueFwd datatype.\n",
    "            \n",
    "            self.pow(3) = self^3\n",
    "        \n",
    "        Args:\n",
    "            self (ValueFwd): ValueFwd.\n",
    "            p (Number): A numerical value.\n",
    "\n",
    "        Returns:\n",
    "            returns ValueFwd f, such that f.v = (self.v)^p and f.d = f' = df/dx\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"v: {self.v}, d:{self.d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7594c5-91b7-4cdb-90f2-912d0c5da30d",
   "metadata": {},
   "source": [
    "# Test forward-mode automatic differentiation\n",
    "\n",
    "Test function 1 is: `test_f1(x)`\n",
    "$$f(x) = \\sin\\left( \\frac{\\sqrt{e^x + 2}}{2}\\right)$$\n",
    "\n",
    "The gradient for Test 1 is: `test_df1`\n",
    "$$\\frac{e^x \\cos\\left( \\frac{\\sqrt{e^x + 2}}{2} \\right)}{4 \\sqrt{e^x + 2}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55291991-2c18-47de-a91f-b330ede140bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_f1(x):\n",
    "    return ((x.exp(math.e) + ValueFwd(2)).pow(0.5)/ValueFwd(2)).sin()\n",
    "\n",
    "def test_df1(x):\n",
    "    ex = math.exp(x)\n",
    "    num = ex*cos(math.sqrt(ex+2)/2)\n",
    "    den = 4 * math.sqrt(ex+2)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca95c0d-aba3-4cee-b033-2acfe8e8c52d",
   "metadata": {},
   "source": [
    "## test Froward-mode Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da485978-28e2-489f-9886-c7d0f06d1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal(test_f1(ValueFwd(1, 1)).d, test_df1(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2f1aa-8220-4c9e-b72a-c1abd715067d",
   "metadata": {},
   "source": [
    "# reverse mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a10f19b-b72b-4b3d-95f6-5f95c76cc693",
   "metadata": {},
   "source": [
    "### Reverse-mode\n",
    "\n",
    "We do two passes in the reverse mode of Automatic Differentiation. One forward and one backward.\n",
    "\n",
    "For a function $f(x) = a(b(c(x)))$. We first calculate the local partial derivatives e.g. $\\frac{\\delta c}{\\delta x}, \\frac{\\delta b}{\\delta c}, \\frac{\\delta a}{\\delta b}$ during the forward pass. and accumulate them in the reverse pass to get $\\frac{\\delta f}{\\delta x}$\n",
    "1. Evaluate $c(x)$ and calculate $c' = \\frac{\\delta c}{\\delta x}$. Store the local gradient $c'$ for future use during the backward pass.\n",
    "2. Evaluate $b(c(x))$. Calculate $b' = \\frac{\\delta b}{\\delta c}$ and store it for future use during the backward pass.\n",
    "3. Similarly evaluate $a(b(c(x)))$ and calculate $\\frac{\\delta a}{\\delta b}$ and store it.\n",
    "4. Initiate the backward pass to accumulate the local gradients and calculate $\\frac{\\delta f}{\\delta x}$ \n",
    "\n",
    "### Implementation\n",
    "\n",
    "Reverse mode is a little more complicated than the forward mode. Here gradients are accumulated during the backward pass. so, we also have to store the computation DAG(Directed Acyclic Graph) so that the backward pass can happen in the correct order.\n",
    "\n",
    "Complete the Python class \"ValueBwd\" to support Reverse-mode Automatic Differentiation as explained above.\n",
    "1. Similar to forward mode store the value of the variable/constant and its gradient w.r.t the independent variable(not updated in the forward pass). Since gradients are accumulated in the backward pass from output-to-input / left-to-right, we have to store the local gradients as well as dependencies i.e., what nodes does the current node depend on? Use `self._prev` list to store the **(input node, and local gradient)** tuples for the current node.\n",
    "2. During the backward pass `backward` recursively propagate the gradients backward and populate `self.d` for the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1c44fb3-b00f-4867-af43-595e8a4ab3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueBwd:\n",
    "    \"\"\" stores a single scalar value and its gradient, updates gradient through back-prop \"\"\"\n",
    "    def __init__(self, data, prev = []):\n",
    "        \"\"\" Value datatype that supports backpropagation on a single scalar variable.\n",
    "\n",
    "        Args:\n",
    "            data (Number): value of the variable/constant.\n",
    "            prev (List[Tuple[ValueBwd, Number]]): List of node(ValueBwd), local gradient tuples that the current node depends on. \n",
    "                Empty list if the computational graph contains a single node.\n",
    "        Returns:\n",
    "            ValueBwd.\n",
    "        \"\"\"\n",
    "        self.v = data\n",
    "        self.d = 0\n",
    "        self._prev = prev\n",
    "\n",
    "    def __add__(l, r):\n",
    "        \"\"\" support for + on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            l (ValueBwd): ValueBwd to the left of +.\n",
    "            r (ValueBwd): ValueBwd to the right of +.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd with updated value and gradient.\n",
    "        \"\"\"\n",
    "        return ValueBwd(l.v + r.v, [(l, 1), (r, 1)])\n",
    "\n",
    "    def __sub__(l, r):\n",
    "        \"\"\" support for - on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            l (ValueBwd): ValueBwd to the left of -.\n",
    "            r (ValueBwd): ValueBwd to the right of -.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __mul__(l, r):\n",
    "        \"\"\" support for * on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            l (ValueBwd): ValueBwd to the left of -.\n",
    "            r (ValueBwd): ValueBwd to the right of -.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __truediv__(l, r):\n",
    "        \"\"\" support for / on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            l (ValueBwd): ValueBwd to the left of -.\n",
    "            r (ValueBwd): ValueBwd to the right of -.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def sin(self):\n",
    "        \"\"\" support for sin(self) on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def cos(self):\n",
    "        \"\"\" support for cos(self) on ValueBwd datatype.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def exp(self, b):\n",
    "        \"\"\" support for b^self on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            b (Number):  A numerical value.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def pow(self, p):\n",
    "        \"\"\" support for self^p on ValueBwd datatype.\n",
    "\n",
    "        Args:\n",
    "            p (ValueBwd):  A numerical value.\n",
    "\n",
    "        Returns:\n",
    "            ValueBwd: Current node of the computational graph.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, d):\n",
    "        \"\"\" Updates local gradient and iteratively calls backward on self._prev.\n",
    "\n",
    "        Args:\n",
    "            d (Number): Gradient Value.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"v: {self.v}, d:{self.d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c141b6-378a-42d8-9605-4e0f14396ec4",
   "metadata": {},
   "source": [
    "## Test reverse mode AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d366ff68-d0a7-45c7-9f7b-db2cdccdf115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_f1(x):\n",
    "    return ((x.exp(math.e) + ValueBwd(2)).pow(0.5)/ValueBwd(2)).sin()\n",
    "\n",
    "def test_df1(x):\n",
    "    ex = math.exp(x)\n",
    "    num = ex*cos(math.sqrt(ex+2)/2)\n",
    "    den = 4 * math.sqrt(ex+2)\n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3cd93c6e-6f88-43ab-a902-be372b33f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 2\n",
    "inp = ValueBwd(x)\n",
    "o = test_f1(inp)\n",
    "o.backward(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a1d4033a-1d9a-4f2a-8ed8-916ff9666448",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_almost_equal(inp.d, test_df1(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d7447b-5bce-4c8c-a8da-839d56692701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
